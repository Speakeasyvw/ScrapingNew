# ?? Scraper de Tiendas Online - Focus Group Recruiter

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://TU-USUARIO-scraper-tiendanube.streamlit.app)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Plataforma automatizada para identificar y extraer datos de contacto de tiendas online usando b?squedas configurables por dominio, pa?s y tipo de negocio. Ideal para reclutamiento de focus groups, investigaci?n y outreach sin limitarte a Tiendanube ni a un nicho espec?fico.

---

## ?? ?Qu? hace esta herramienta?

Esta aplicaci?n web permite:

- ? **Construir b?squedas autom?ticas** con operador `site:` usando tu dominio objetivo, pa?s y tipo de negocio
- ? **Extraer autom?ticamente** datos de contacto de las URLs encontradas
- ? **Encontrar** Instagram, Email, WhatsApp y Facebook
- ? **Clasificar** tiendas por nicho e inter?s (moda, belleza, hogar, tecnolog?a, etc.)
- ? **Priorizar** contactos por calidad de datos (scoring autom?tico)
- ? **Exportar** CSVs listos para campa?as de outreach
- ? **Visualizar** estad?sticas y m?tricas en tiempo real

---

## ?? Modos de b?squeda

1. **?? Buscar URLs en Google:** selecciona dominio (ej. `mitiendanube.com` o `shopify.com`), pa?s, perfil de negocio y palabras clave adicionales para generar la consulta. Compatible con Serper, SerpAPI o DuckDuckGo.
2. **?? Ingresar URLs manualmente:** pega cualquier listado de URLs que ya tengas identificado.
3. **?? Subir archivo `.txt`:** importa un archivo con una URL por l?nea para procesar lotes grandes.
4. **?? Usar URLs de prueba:** ideal para demos o tests r?pidos.

---

## ğŸŒ Demo en Vivo

**ğŸ”— [Abrir App](https://scrapingnew.streamlit.app/)**


---

## ğŸ“Š Resultados Esperados

| MÃ©trica | Tasa de Ã‰xito |
|---------|---------------|
| ğŸ“· Instagram | 70-90% |
| ğŸ“± WhatsApp | 40-60% |
| ğŸ“§ Email | 20-40% |
| ğŸ‘¥ Facebook | 30-50% |

**Ejemplo:** De 100 tiendas scrapeadas, obtendrÃ¡s aproximadamente:
- 80+ perfiles de Instagram
- 50+ nÃºmeros de WhatsApp
- 30+ emails
- 40+ pÃ¡ginas de Facebook

---

## ?? Uso R?pido

### Opci?n 1: Buscar URLs en Google (autom?tico)

1. **Abre la app:** https://scrapingnew.streamlit.app/
2. **Elige ??? Buscar URLs en Google? en la barra lateral.**
3. **Configura la b?squeda:** dominio objetivo, pa?s, tipo de negocio y palabras clave extra (o usa tu consulta personalizada).
4. **Selecciona el servicio:** Serper, SerpAPI o DuckDuckGo.
5. **Haz clic en ??? Buscar URLs? y luego en ??? Iniciar Scraping?.**
6. **Descarga tus CSVs:** completo y Top 50 ordenados por score.

?? **Tiempo estimado:**
- 10 tiendas: ~2 minutos
- 50 tiendas: ~10 minutos
- 100 tiendas: ~20 minutos

### Opci?n 2: Ingresar o importar URLs existentes

1. Selecciona ??? Ingresar URLs manualmente? o ??? Subir archivo .txt?.
2. Pega/importe tus URLs (una por l?nea).
3. Presiona ??? Iniciar Scraping? y espera la barra de progreso.
4. Descarga los CSVs generados autom?ticamente.

### Opci?n 3: Ejecutar Localmente

```bash
# Clonar repositorio
git clone https://github.com/TU-USUARIO/scraper-tiendanube.git
cd scraper-tiendanube

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar app
streamlit run app.py
```

La app se abrir? autom?ticamente en `http://localhost:8501`

---

## ğŸ“ Estructura del Proyecto

```
scraper-tiendanube/
â”œâ”€â”€ app.py                 # AplicaciÃ³n principal Streamlit
â”œâ”€â”€ requirements.txt       # Dependencias Python
â”œâ”€â”€ README.md             # Esta documentaciÃ³n
â”œâ”€â”€ .gitignore            # Archivos ignorados por Git
â””â”€â”€ LICENSE               # Licencia MIT
```

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **[Streamlit](https://streamlit.io)** - Framework de UI
- **[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/)** - Web scraping
- **[Pandas](https://pandas.pydata.org/)** - Procesamiento de datos
- **[Requests](https://requests.readthedocs.io/)** - HTTP requests

---

## ğŸ“‹ CaracterÃ­sticas

### ?? BÃºsquedas configurables
- Dominio personalizable usando `site:`
- Filtros rÃ¡pidos por paÃ­s y perfil/nicho
- Palabras clave adicionales o consulta manual
- Compatible con Serper, SerpAPI y DuckDuckGo

### âœ¨ ExtracciÃ³n de Datos
- Nombre de la tienda
- URL completa
- DescripciÃ³n (primeros 200 caracteres)
- Nicho automÃ¡tico (10+ categorÃ­as)
- Redes sociales (Instagram, Facebook)
- Datos de contacto (Email, WhatsApp)
- Score de calidad (0-8 puntos)

### ğŸ“Š AnÃ¡lisis y VisualizaciÃ³n
- MÃ©tricas en tiempo real
- GrÃ¡ficos por nicho
- DistribuciÃ³n de scores
- Filtros interactivos
- Tablas ordenables

### ğŸ“¥ ExportaciÃ³n
- **CSV Completo:** Todos los datos
- **CSV Top 50:** Las 50 mejores tiendas por score
- Formato UTF-8 compatible con Excel
- Nombres de archivo con timestamp

---

## ğŸ¯ Sistema de Scoring

Cada tienda recibe un **score de contacto** de 0 a 8 puntos:

| Dato Encontrado | Puntos |
|----------------|--------|
| Instagram | +3 |
| WhatsApp | +2 |
| Email | +2 |
| Facebook | +1 |

**Ejemplo:**
- Tienda con IG + WA + Email = 7 puntos â­â­â­
- Tienda con solo IG = 3 puntos â­
- Tienda sin contactos = 0 puntos (descartada)

---

## ğŸ“– GuÃ­a de Uso Detallada

### 1ï¸âƒ£ Preparar tus URLs

Tienes 3 opciones:

**A) Ingresar manualmente**
```
https://beautymakeup.mitiendanube.com
https://modafashion.mitiendanube.com
https://joyasarte.mitiendanube.com
```

**B) Crear archivo .txt**
```
# urls_input.txt
https://tienda1.mitiendanube.com
https://tienda2.mitiendanube.com
https://tienda3.mitiendanube.com
```

**C) Usar URLs de prueba**
- La app incluye 5 URLs reales para testing

### 2ï¸âƒ£ Configurar Scraping

En la barra lateral puedes ver:
- Modo de ingreso seleccionado
- EstadÃ­sticas esperadas
- InformaciÃ³n de la app

### 3ï¸âƒ£ Ejecutar y Monitorear

Durante el scraping verÃ¡s:
- Barra de progreso
- Tienda actual siendo procesada
- Resultados en tiempo real:
  - âœ“ = Dato encontrado
  - âœ— = Dato no encontrado

### 4ï¸âƒ£ Analizar Resultados

DespuÃ©s del scraping verÃ¡s:
- **MÃ©tricas:** Total, con IG, con email, con WA
- **GrÃ¡ficos:** DistribuciÃ³n por nicho y score
- **Tabla:** Datos completos con filtros

### 5ï¸âƒ£ Exportar Datos

Descarga los CSVs:
- **Completo:** Para anÃ¡lisis exhaustivo
- **Top 50:** Para priorizar outreach

---

## ğŸ” Nichos Detectados AutomÃ¡ticamente

La app clasifica las tiendas en:

- ğŸ’„ **Belleza - Maquillaje:** CosmÃ©ticos, labiales, sombras
- ğŸ§´ **Belleza - Skincare:** Cremas, serums, faciales
- ğŸ’ **JoyerÃ­a:** Oro, plata, anillos, collares
- âœ¨ **Bijouterie:** Accesorios de fantasÃ­a
- ğŸ‘— **Moda - Ropa:** Vestidos, blusas, indumentaria
- ğŸ‘œ **Accesorios:** Carteras, bolsos, cinturones
- ğŸ **Otro:** CategorÃ­as no especificadas

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Ajustar Velocidad de Scraping

Edita `app.py` lÃ­nea 48:

```python
# MÃ¡s lento pero mÃ¡s seguro (menos riesgo de bloqueo)
time.sleep(2.0)

# MÃ¡s rÃ¡pido pero mayor riesgo
time.sleep(0.8)

# Balance recomendado
time.sleep(1.5)  # Default
```

### Cambiar Timeout de Requests

LÃ­nea 35:

```python
response = self.session.get(url, timeout=15)  # 15 segundos
```

---

## ğŸš¨ Limitaciones y Consideraciones

### TÃ©cnicas
- âŒ No funciona con tiendas que requieren login
- âŒ JavaScript pesado puede no cargar completamente
- âš ï¸ Rate limiting: 2 segundos entre requests
- âš ï¸ Timeout: 15 segundos por tienda

### Legales
- âœ… Solo datos pÃºblicos disponibles en web
- âœ… Respeta `robots.txt` de Tiendanube
- âŒ No usar para spam o acoso
- âŒ No revender datos personales

### Ã‰ticas
- ğŸ¯ Uso exclusivo para research/focus groups
- ğŸ¤ Contacto respetuoso y transparente
- ğŸ“§ Ofrecer opt-out en comunicaciones
- ğŸ›¡ï¸ Proteger privacidad de datos

---

## ğŸ“ˆ Roadmap y Mejoras Futuras

- [ ] Soporte para Selenium (tiendas con JS)
- [ ] IntegraciÃ³n con Hunter.io API (emails)
- [ ] VerificaciÃ³n de Instagram (seguidores, actividad)
- [ ] Export a Google Sheets
- [ ] BÃºsqueda integrada en Google
- [ ] DetecciÃ³n de duplicados inteligente
- [ ] HistÃ³rico de scraping
- [ ] AutenticaciÃ³n de usuarios
- [ ] Scheduler para scraping automÃ¡tico
- [ ] API REST

---

## ğŸ¤ Contribuir

Â¿Quieres mejorar el scraper? Â¡Contribuciones son bienvenidas!

1. Fork el proyecto
2. Crea tu rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit cambios: `git commit -m 'Agrega nueva funcionalidad'`
4. Push a la rama: `git push origin feature/nueva-funcionalidad`
5. Abre un Pull Request

---

## ğŸ› Reportar Bugs

Si encuentras un error:

1. Ve a [Issues](https://github.com/speakeasyvw/ScrapingNew/issues)
2. Crea un nuevo issue
3. Incluye:
   - DescripciÃ³n del error
   - Pasos para reproducir
   - Screenshots (si aplica)
   - Sistema operativo y versiÃ³n de Python

---

## ğŸ“ Soporte

- ğŸ“§ **Email:** duartedeveloping@gmail.com
- ğŸ’¬ **GitHub Issues:** [Abrir issue](https://github.com/speakeasyvw/ScrapingNew/issues)

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

```
MIT License

Copyright (c) 2025 Giovanni Duarte

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸŒŸ Reconocimientos

Desarrollado para facilitar el reclutamiento de focus groups en el ecosistema de e-commerce latinoamericano.

**TecnologÃ­as de cÃ³digo abierto utilizadas:**
- Streamlit Community Cloud
- Beautiful Soup
- Pandas
- Python

---

## ğŸ“Š EstadÃ­sticas del Proyecto

![GitHub stars](https://img.shields.io/github/stars/Speakeasyvw/ScrapingNew?style=social)
![GitHub forks](https://img.shields.io/github/forks/Speakeasyvw/ScrapingNew?style=social)
![GitHub issues](https://img.shields.io/github/issues/Speakeasyvw/ScrapingNew)
![GitHub last commit](https://img.shields.io/github/last-commit/Speakeasyvw/ScrapingNew)

---

## ğŸ“ Casos de Uso

### Research y Academia
- Estudios de mercado en e-commerce
- AnÃ¡lisis de emprendedoras digitales
- InvestigaciÃ³n de ecosistemas startup

### Reclutamiento
- Focus groups de productos
- User research para apps
- Beta testers de herramientas

### Marketing
- Base de datos para outreach
- IdentificaciÃ³n de influencers
- AnÃ¡lisis de competencia

---

## âœ… Checklist de Setup

Para nuevos usuarios:

- [ ] Python 3.8+ instalado
- [ ] Git instalado (opcional)
- [ ] Cuenta de GitHub creada
- [ ] Repositorio clonado/forked
- [ ] Dependencias instaladas
- [ ] App probada localmente
- [ ] Deploy en Streamlit Cloud completado
- [ ] URL personalizada configurada
- [ ] README personalizado con tu info

---

## ğŸš€ Deploy RÃ¡pido

```bash
# 1 minuto setup
git clone https://github.com/Speakeasyvw/ScrapingNew.git
cd ScrapingNew
pip install -r requirements.txt
streamlit run app.py

# Deploy en cloud: https://share.streamlit.io
# Connect GitHub â†’ Select repo â†’ Deploy!
```

---

## ğŸ’¡ Tips y Trucos

### Mejorar Tasa de Ã‰xito de Emails
1. Usar Hunter.io despuÃ©s del scraping
2. Buscar en bio de Instagram
3. Revisar pÃ¡gina de contacto manualmente

### Optimizar Velocidad
- Scrapear en lotes de 50-100
- Usar URLs de alta calidad
- Filtrar por nicho antes de scrapear

### Mantener Datos Actualizados
- Re-scrapear cada 2-3 meses
- Verificar Instagrams activos
- Validar emails periÃ³dicamente

---

## ğŸ‰ Â¡Empecemos!

**ğŸ‘‰ [Abrir App en Streamlit](https://scrapingnew.streamlit.app/)**

---

<div align="center">

**Hecho con â¤ï¸ para la comunidad de e-commerce**

â­ Si te fue Ãºtil, deja una estrella en el repo

</div>
